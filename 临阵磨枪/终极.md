## 对线性代数的理解

#### 线性变换

数学上：

线性变换是一种函数，对加法和数乘满足一定的性质。
$$
A(a+b) = A(a) + A(b)\\
A(ka) = kA(a)
$$
直觉上：它既然叫变换，其实就是在暗示我们通过图像来认识他。尤其是借助二维图像。

线性变化对整个空间做了变换，但是变换之后网格线等距且平行，原点保持不动。

变换之后，原来空间的一个向量，就变成了另一个向量。

那我们的问题就变成了如何描述线性变换。

#### 脑海中想象线性变换

由于任意向量都可以表示成ai + bj，由于线性变换的性质，
我们只要追踪i，j的变化，就能知道变化后的v。

比如说 i -> (2,0), j->(-1,1) ,  (1,2) -> 1* (2,0) + 2*(1,1)。

那现在已经知道如何一种线性变换对向量的影响，只要通过基向量就能看出来。那么我们就可以使用这两个基向量，来唯一标识这个线性变换。所以把两个新的基向量写出来，就能表示出该线性变换 。

#### 矩阵的含义

所以说，矩阵是一种线性变换，或者说它是线性变换的表述方式。

矩阵 * 向量， 得到线性变换后的向量。

直观上证明不符合交换律，但是符合结合律。

#### Rotate Shear Scale

非常直观。现在你能直接写出这些矩阵了

只要分别知道Rotate Shear Scale 分别是基向量变成了什么就行了。

#### 行列式

线性变化带有放缩性质，我们想知道变换之后，面积scale了多少。

对于面积来说，我们只要考虑单位正方形从一开始到变换后scale了多少就行了。

这个放大的比例，就是【行列式】，

行列式为负数，实际上代表表面反转了。

行列式为0时，直接压缩成了一条直线，甚至一个点。

所以行列式为0时，实际上压缩维度。

三位空间中的行列式，就是【单位正方形体积的变换】。为零时，体积变为0。负数代表右手定则是否保持。

|D1D2| = |D1||D2|

#### 逆矩阵

$$
Ax = y
$$

代表向量x在A的线性变换作用下，变成了向量y。

对行向量y逆向线性变换，得到x。A和A逆效果是相互抵消的，也就是基向量都不发生变化。

矩阵行列式为0时，你不能进行逆变换。

因为有很多高维度的向量被压缩到同一个低维向量，一个逆变换就会让一个低维向量对应多个高维向量，这是【函数】所不允许的。

#### 内积

#### 基变换

大家用各自的语言去描述各自空间里的向量。不同的空间有不同的基向量。

他人的基向量ex、ey，对他自己来说，是(1,0) , (0,1)，但是在我们看来，是i',j'，而我们的基向量是(1,0),(0,1)。

因此在我们的空间里描述他人的向量
$$
 \left [ \begin{array}{}
            x_{me} \\
            y_{me}\\
	\end{array} 
\right]

= \left[i',j' \right] 
\left [ \begin{array}{}
            x_{him} \\
            y_{him}\\
	\end{array} 
\right] \\
也就是\ C_{me} =\ \left[i',j' \right] C_{him}
$$
那已知我们世界里的坐标，如何得到他的世界里的坐标？

只要对这个线性变换做逆变换就可以了
$$
C_{him}=A^{-1}C_{me}
$$


#### 特征向量与特征值

部分特殊的向量，在线性变化后，仍留在原来的直线上。
$$
Ax = \lambda x
$$
表明，x在变换之后，方向仍然保持不变，特征值λ其实是它scale的倍数。

对于三维旋转矩阵来说，方向不变的向量，就是它的旋转轴。

绕轴旋转，相比于三维矩阵。对于人来说，更能理解

顺带一提，非常像Quaternion

**我们可以不通过空间变换来理解线性变换，我们可以通过特征向量来理解线性变换**

因为特征向量前后方向不变，特征值就是放缩倍数。这也就是为什么特征值之积等于行列式。

求解特征向量、特征值
$$
(A  -\lambda E)x = 0
$$
只有行列式为0时，才能把一个非0向量压缩成0.

为什么要对角化，因为对角化的矩阵基向量就是特征向量，对角矩阵的n次，就是scale了n次。
$$
P^{-1} AP
$$
P是特征向量组成矩阵，在A的作用下，只会scale，再用P-1,抵消掉P的作用，只剩下了scale。

## 从线性代数的可解释性到深度学习的可解释性。

## 过拟合与欠拟合

我们希望深度学习或者机器学习的模型对于数据训练集有较好的拟合，但也希望它能够对于测试训练集有比较好的拟合能力，也就是泛化能力。泛化能力上出现问题有两种情况：过拟合与欠拟合。

一开始随着训练增加，训练误差和泛化误差都有所减小，但是到某个点后，训练误差减小，泛化误差却有所增加。

#### 欠拟合

模型复杂度低，在训练集上都没有很好的拟合效果。

解决方法：训练时间久些基本上不会发生，增加网络复杂度。

#### 过拟合

模型复杂度太高，泛化误差太大。相当于模型对训练集死记硬背。没有理解数据背后的规律。

原因——样本和模型

1. 训练数据样本单一，缺乏某一类型的样本。
2. 训练数据噪声太多，影响了测试输入和输出
3. 模型太复杂，学习了太多训练集特征，对训练数据点接近拟合。但遇到新数据点时，泛化能力就不行了。

解决方法：

宗旨：要么提升数据质量；要么降低系统复杂度。

1. 正则化，L0/L1/L2正则化。
   L0 非零个数，L1绝对值之和，使得参数尽可能0，更加稀疏。
   L2模长不开根号。使得参数趋近0但不等于0。【相比于原来更小（因为BP后参数乘以了一个小数）】

2. 提前终止，Dropout等

   **提前终止**：发现泛化误差开始提高就截止。
   **Dropout**：神经元以一定概率不参加运算，减弱了神经元之间的联合，从而增强了泛化能力。

3. 对样本进行优化（增多样本，样本正例反例贴近真实分布）。
   现实情况当然没有那么多样本，所以一般进行数据增强。对样本进行水平翻转、旋转、裁剪、噪声等等。

解决办法：

1. CNN：卷积核参数共享，提取特征。
2. 决策树：如果效果在测试集上分类能力没有提升就剪枝
3. SVM:  软间隔（允许犯错）和正则化（引入L2范数）

## CNN经典网络架构

#### AlexNet（LeNet5更早）

<img src="C:\Users\卫清渠\AppData\Roaming\Typora\typora-user-images\image-20230920200954829.png" alt="image-20230920200954829" style="zoom:50%;" />

缺点：11*11 filter太大，5层Conv，FC3层。

数据增强的方法：

1. 镜像反射，同一张图片发射。
2. 同一张图片进行裁剪，最后取平均

其它优点：

1. 使用了Relu，偏导为1，好计算；
2. LRN，局部相应归一化，让反馈大的权值变得更大，并且抑制了反馈较小的神经元。
3. Dropout，在一定概率下某些神经元置为0，好像这个神经元被删除了一样。
4. 使用了Maxpooling
5. 双GPU训练，工程上的创新。

#### VGG网络

1. 卷积核3*3 大小，用三个3\*3替代1个7\*7，用两个3\*3替代1个5\*5.
   参数更少，但是感受野一样。
2. 池化后保留feature Map数量翻倍保留更多信息

精度更高但是训练时间更长【使用了太多卷积层而且深度非常大】了。

发现16层网络比30层网络要更好。

## Resnet

问题：深层次网络效果不比层数少的好。

![image-20230920212404000](C:\Users\卫清渠\AppData\Roaming\Typora\typora-user-images\image-20230920212404000.png)

训练的时候，某些层对最终loss只有副作用，神经网络会倾向于把这层的参数训练成0；这就相当于这层没有参与训练。

![image-20230920212846926](C:\Users\卫清渠\AppData\Roaming\Typora\typora-user-images\image-20230920212846926.png)

a-1层的输出作为a+1层的输入，哪怕a层特征提取效果差，weight被置为0，a-1层的输出依旧能到达a+1层，这样就能求导了。

#### 论文精读 Deep Residual Learning for Image Recognition

在CIFAR-10 上发现56层普通网络比20层普通网络效果更差。

34层resnet效果比18层更好。

#### Intro

深的网络不同level可以获得不同层次的特征。

但是网络越深就会越来越好吗？

遇到的问题：

1. 梯度爆炸或消失 -> 加入Batch Normalization
2. 精度反而变差 ，train&test erro同时上升-> 甚至都不是overfitting

**按理来说深层不应该比浅层差，因为深层完全可以作identity mapping ，但是SGD从实际上来看无法实现。**

解决方法: 添加shortcut

优点：不会添加任何参数。

F（x） 与 x 格式不等：添加0 / 投影，

添加了Residual像使得求导始终保留一个浅层网络的大项，使得能够训的动网络，否则随着梯度加深，导数趋向于0。

导数趋向于零，参数不太更新。



直观上看：加了残差网络，模型的复杂度反而降低了。



#### 感受野

Feature Map 中一个神经元所对应的原来神经元的数量。

可以代表这个神经元提取特征的范围大小。

假设(2k+1) * (2k+1)的卷积核，再假设卷积层有N层。

记2k+1 =t 

| 层号 | 感受野大小                    |
| ---- | ----------------------------- |
| 1    | t * t                         |
| 2    | (2k+ t) * (2k + t)            |
| 3    | (4k+ t) * (4k+t)              |
| ...  | ...                           |
| N    | (2(N-1)k + t) * (2(N-1)k + t) |

多层小卷积核代替少层大卷积（VGG的思想）

前者参数更少，提取细致，并且由于Relu更多，加入的非线性变换也更多。

#### Batch Normalization 

问题：Out = Wx + b，x的分布随着网络加深过程当中，分布逐渐偏离。分布偏离导致激活函数（Sig，tanh）取值王上下两端靠近，梯度减小，导致更新缓慢。

BN通过调整梯度分布，使得计算导数时输入落在比较敏感的区域。

各个特征x1,x2,x3的范围有所不同，可能有些特征范围取值特别大。

正则化：
$$
X_i = \frac{X_i-Mean_i}{StdDev_i}
$$
将特征的值变成服从标准正态分布。

防止大特征。因为大特征乘上weight淹没小特征。

## Data Augmentation

数据增强

1. 水平反转
2. 旋转
3. 裁剪
4. 缩放

## Fine Tune 和 Pretrain

预训练模型pretrained model：已经训练好了的模型

在此基础上训练，称之为【微调】Fine Tune。相比于从头训练减少了很多时间。

#### 为什么还要微调？

浅层卷积层提取基础特征，深层卷积层提取抽象特征（脸型，无法用语言描述），全连接层根据特征组合进行评分分类。

使用了大型数据集做训练，已经具备了提取浅层基础特征和深层抽象特征的能力。

此外

1. 避免从头训练，节省了时间
2. 存在模型不收敛，参数不够优化，泛化能力低等风险。

#### Fine Tune注意点

使用的数据集和预训练的数据集相似

