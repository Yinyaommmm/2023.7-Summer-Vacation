## 机器学习

通过计算手段，利用经验（数据），改善系统自身性能。

也是从数据上产生模型的算法。

数据集：样本的集合，每个样本都有若干个attribute或feature组成。

学习：使用某种算法，使用训练数据集找到数据内涵的潜在规律，逼近ground truth。

标记label：带有最终结果的样例。

问题的分类：离散——分类，连续——回归。

最终目标：希望从训练集中获取的模型最终能够泛化到整个样本空间中。



**归纳：**从事实中总结出一般性规律。从特殊总结出一般。

**演绎：**基于公理和推导规则推导出相恰的定理。从一般运用到特殊。

**假设：**由机器学习学得的规律（模型），但这不一定就是真相。同样的学习算法，不一样的参数配置，会带来不一样的模型结果（假设）

**假设空间：** 所有模型能学成的结果都属于该空间，比如假设空间 = 一元函数、二次函数等，也有可能是离散的假设空间。

**归纳偏好**：**假设与训练集一致，但在面对测试样例时就会有差别。**不同学习算法会产生不同的假设，比如相信更平滑的拟合。没有一个针对任何问题都更优的学习算法。

NFL定理：学习算法在一类上问题更优，就会在另一类问题上更差。总误差是一定的。所以必须要针对一个具体的问题，使用在这个问题上表现良好的学习算法。

NFL告诉我们，要针对具体的问题使用特殊的学习算法，你没法找到一个泛用的学习算法。

## 人工智能的发展

| 阶段 | 时间      | 成果                                         | 主要方法           | 缺点                   |
| ---- | --------- | -------------------------------------------- | ------------------ | ---------------------- |
| 一   | 50~70年代 | 逻辑理论家程序、通用问题求解程序             | 推理能力、灌入知识 | 难以将人的知识总结出来 |
| 二   | 50~70年代 | 连接主义：感知机<br />符号主义：结构学习系统 | 连接主义、符号主义 | 需要算力、数据         |



符号主义：用一阶逻辑进行知识表示，基于用符号表示知识，通过演绎和推理获得结果。但是学习过程中面临的假设空间太大，复杂度极高，难以有效学习。

连接主义：基于神经网络，产生的结果是个黑箱，不像符号主义那样明确。缺点是学习过程中涉及大量参数，而参数的设置全靠手工调整，但是参数调节上失之毫厘又会谬以千里。

统计学习：代表技术Support Vector Machine以及Kernel Methods。统计学习和连接主义有密切联系。

21世纪连接主义以“深度学习”之名卷土重来。深度学习狭义来说就是：很多层的神经网络。

深度学习发展背景：大数据、高算力。对大数据的分析与规律应用。



数据挖掘与机器学习：数据挖掘收到DB,ML,统计学三者影响。DB提供数据管理技术，ML和Statistics提供数据分析技术。



其它应用：搜索引擎，天气预测，环境监测，图片搜索，自动驾驶 甚至投票选举。

## 第二章 模型评估与选择

**过拟合**overfitting：将训练样本自身的特点当作了所有样本的特点（比如识别猫狗，训练图片的背景都是绿色的，有可能将绿色背景认作是猫狗图片的特征）。【也就是学习能力太强】

我们难以获得泛化误差，而训练误差容易产生过拟合现象。

#### 评估方法

**留出法：** 将数据集D 划分为互斥的training set和test set。但要保证各个标记的分布一致（正例/反例）

**交叉验证法：**将D分成互斥的数据分布一致的k个子集，k-1个作training set，1个作测试。总共k轮。

**自助法：**每次都从D中随机挑一个加入D‘，大概有36.8%的样本未出现在D中。使用D\D'作为training set，D’作为训练集。

#### 性能估计

将模型预测结果与真实标记y进行比较。需要有一些指标

**查准率：**（判断为真且正确 占 总判断为真数量的多少）。正确判断好瓜的能力。

**查全率：**（认为是真的，占总真的多少），判断出的好瓜占实际好瓜数量的多少

两者应用场景不同：前者适用于推荐；后者不希望错漏。

**F1 ：**综合查准率和查全率对模型的评价。本值是调和平均（的倒数），较小项会被凸显，导致F1偏小。

是否判定为真或假，取决于我们设定的阈值。比如高于0.5为正，小于0.5为假。我们需要找出一个比较好的阈值。

**ROC曲线**：
纵轴真正利率： TP / TP+FN 你认为真的且正确 / 正例；（查全率）
横轴假正利率： FP / TN+ FP，你认为的真的但是猜错了 / 反例。

一条ROC包裹住另一条ROC，则更优。因为越好的学习器查全率越高，猜错比例越小，整条曲线越往左上方靠。

**AUC** area under curve：ROC的面积

#### cost sensitive

对于医学领域，将正常人判断为患病代价更低。但是将病人判断为正常人，代价高昂。因此我们为不同的判断错误赋予不同的代价。

## 第三章线性模型

$$
\boldsymbol f(\boldsymbol x) = \boldsymbol w^T\boldsymbol x+  b
$$

使用MSE作为loss function。求MSE最小化的过程就是最小二乘法。

## 第四章决策树

递归进行的一个过程。

Function：Generate Tree(Dataset D(X,y) , Attribute A)

【叶子，C】：代表该节点为叶子，类别为D中最多的类别C。

1. 生成一个Node
2. 如果D中所有数据的y相同（类别相同），【叶子，C】，return
3. 如果A == 空集，已经没法继续递归了，【叶子，C】，return
4. 如果D中在A上的取值相同，【叶子，C】,return 
5. 挑选一个最优划分属性a*（使得每个类中最大类的占比更大），基于a\*的值进行划分
6. 【遍历过程中】a*i为属性值，Dv为该a\*取值为a\*i的训练集.
7.  如果Dv为空，说明该分支上没有足够的参数，那么盲猜，【叶子，C】
8. 不为空则递归GenerateTree（Dv，A\a\*）

#### 核心问题：如何进行划分

目的：使得最后判定类别时，最大类占比更大，【纯度】更高。

信息熵：所有类别的熵求和。
$$
Ent(D)= -\sum_{i=1}^{|y|} p_i\log(p_i)
$$
Ent越小，纯度越高。（当一个分类概率为1时，Ent=0）

熵代表着不确定性，如果p=1，那么就没有不确定了，**这是我们向往的**。如果均匀分布，熵将达到最大。

依据某个标签分类，分出D1，D2，D3... ，所有分支求信息熵之和再加个数的权重。可以进行比较，

#### 信息增益Information Gain：

$$
Gain(D,a) = Ent(D) - \sum \frac{|D_v|}{D}Ent(D_v)
$$

EntDv越小越好，因此Gain(D,a)越大越好。

缺点：对多值划分的属性比较有偏好。

#### 增益率 Gain Ratio

IV(a)作为属性a的固有值，基于属性划分值数量，划分越多，值越大。

用Gain Ratio = Gain / IV(a) 得到最终评判结果，对可取值数目较少的属性偏好。

两种方式杂糅：先选出信息增益高于平均的，再选择增益率最高的。

#### 纯度的另一种定量表示：Gini指数

$$
Gini(D) = 1 - \sum_{i=1}^{i=|y|} p_i^2
$$

也就是任选两个样本，样本不一样的比率。不一样的比率越小，Gini越小，纯度越大。

#### 剪枝

决策树学习法对付过拟合【学习能力太强，把训练样本的一些性质当成可以泛化的性质了】

表现在决策树的分支特别多。因而需要一些剪枝。关键在于如何判断剪枝能否带来泛化能力提升？

利用测试集进行验证：比较划分前后对测试集预测的正确率。

**预剪枝：边划分，边考虑剪枝**划分前：该节点类别为D中最大类别，计算一下预测正确率。划分后，每个子节点类别为该节点Dv中最大类别。

优点：减少了分支展开的数目，提高了学习效率，降低了训练时间。

缺点：这是基于贪心的剪枝，有可能将来在此基础上进行的划分能够带来泛化能力的提高，但被我们强行剔除了。存在着欠拟合的风险（对训练集学习的不充分）。

**后剪枝：生成完树后再剪枝**

优点：学习完毕后再剪枝，欠拟合风险小

缺点：自底向上多所有非叶子节点进行考察，代价非常大。

### 连续值的决策树

针对测试集在属性A上分类，所有训练数据A的值a进行从小到大排序，某两个点的中位数当作候选划分点。然后就可以像离散属性值那样来考察选取最优划分点。

我们要找到一个划分点，使得Gain（D，a，t）最大。按照一个t划分后，会分成+ - 两类，

计算|D+|* Ent（D+） + |D-|*Ent(D-)，选取一个最小的。

#### 缺失值

鉴于隐私保护、成本等因素，可能数据的某些值会缺失。问题：

1. 属性值有缺失，如何选择划分属性。
2. 依据属性A进行划分，但数据在A上缺失值。

## 第五章：神经网络

神经网络：是具有适应性的简单大院组成的广泛并行互联的网络，模拟生物神经系统对世界物体所做出的交互反应。

![image-20230915000819790](C:\Users\卫清渠\AppData\Roaming\Typora\typora-user-images\image-20230915000819790.png)

输入信号与权重相乘，并与阈值theta比较，通过激活函数f进行处理。理想的激活函数是阶跃函数，但是它不光滑，不连续，改为使用Sigmoid，将R -> [0,1]

![image-20230915000942821](C:\Users\卫清渠\AppData\Roaming\Typora\typora-user-images\image-20230915000942821.png)

#### 感知机

只有两层神经元（输入+输出），可以模拟与或非运算。感知机模型公式表示：
$$
y = f(\sum_{i=1}^{n}\omega_ix_i - \theta) = f(\boldsymbol \omega^{\boldsymbol T}\boldsymbol x - \theta)
$$
f如果是阶跃函数，将wTx-θ看作n维空间的超平面，n维空间被划分为两个子空间，样本落在一个子空间y=1，落在另一个子空间y=0，如此实现分类。

#### 感知机的训练：

$$
Loss\ Function = L(\omega,\theta) = \sum_{x\in ErrorSet}(\hat y-y)(\omega^Tx-\theta) \\
可以将\theta并入\omega_{n+1},x_{n+1}=-1,求L关于\omega的梯度\\
 梯度\boldsymbol \omega = (\hat y-y)\boldsymbol x
$$

训练方法：加上学习率η*反向梯度
$$
训练样例(X,y)，预测值为\hat y \\
\omega_i = \omega_i + \Delta\omega_i \\
\Delta\omega_i  = \eta(y-\hat y)x_i,
$$
只要问题线性可分，那就能用感知机解决。但是像“异或”那样不能用一个超平面分类的问题，一层感知机就不行了。

![image-20230915003913316](C:\Users\卫清渠\AppData\Roaming\Typora\typora-user-images\image-20230915003913316.png)

解决非线性问题，需要多层功能神经元。

输入层和输出层中间的神经元层叫做隐含层，隐含层和神经元都有激活函数。

【多层前反馈神经网络】：层内不互联，连接不跨层。

#### BP算法

链式求导，注意Sigmoid函数求导的有趣性质。f'(x) = f(x)(1- f(x))

以三层为例，输入层d个神经元，隐含层q个神经元，输出层l个神经元。

共有qd+q +ql+l个参数。（bias+weight）



可以选择读入一次数据(xk,yk)就进行一次更新【SGD】，也可以读入全部后取平均误差进行更新【Batch GD】，还可以小批量进行更新【Mini GD】.

后者参数更新少些，

[Hornik et a l., 1 9 8 9 ]证明了只要有一个足够多神经元的隐层，神经网络就能以任意精度逼近任意复杂度的连续函数。



由于强大的表示能力，BP神经网络经常过拟合，训练误差降低但是测试误差上升。

#### 局部最小和全局最小

局部最小：在领域之内，其函数值最小。

如何跳出局部最小，寻找全局最小：

1. 多组不同参数值初始化神经网络，可能会陷入不同的局部最小，选择更有利结果。
2. 模拟退火，每一步有一定概率选择次优解。但是选择次优解的概率随着时间推移而降低。
3. 随机梯度下降，计算梯度时引入随机因素。

#### 其它神经网络

我们之前学神经网络叫做multi-layer feedforward neural network.多层前向反馈神经网络。FNN。同层没有连接，也没有跨层连接。

###### RBF网络

radial basis function

三层，输出等于
$$
\varphi(\boldsymbol x)=\sum_{i=1}^{q} \omega_i\rho(\boldsymbol x,\boldsymbol c_i)\\
\rho(\boldsymbol x,\boldsymbol c_i)=e^{-\beta_i||\boldsymbol x - \boldsymbol c_i||^2}
$$
输出层是对隐含层神经元的线性组合。

隐含层激活函数是径向基函数。可以看出径向基函数围绕Ci【可以看作中心点】的球面上的点函数值相同。

也被证明了可以拟合任意函数。

###### ART网络

Adaptive Resonance Theory，自适应谐振理论网络。

竞争性学习代表，输出神经元相互竞争，每个时刻只有唯一胜者神经元被激活。

###### Elman网络

让一些神经元的输出反馈回来作为输入信号。

###### 玻尔兹曼机

神经元都是布尔型的，只有0、1两种状态。

一种基于能量的模型。

#### 深度学习

一般来说参数越多，模型复杂度越高，模型能完成更复杂的学习任务，但训练效率低，容易陷入过拟合。

云计算训练效率低的问题；大数据（更多的训练数据）解决了过拟合的问题。

云计算包括

1. IaaS(Infrastructure as a service)：提供用户能够部署和运行的硬件（CPU,内存）等。
2. Pass（platform）：提供应用程序开发环境
3. Saas（Software）：直接提供开发好的软件，用浏览器就能打开软件。



典型的深度学习是很深层的神经网络，通过增加隐层层数来提高模型复杂度。

CNN： Convolutional Neural Network，采用了权共享策略。 

多隐层堆叠，每层对上一层输出进行处理的机制，可以看作是在对上层信号进行加工。

把原始的、与输出目标联系不太密切的输入表示，转化成与输出目标联系更密切的表示。

是原来仅基于最后一层输出映射难以完成过的任务成为可能。

换言之，将”低层“特征转化为”高层特征“，用简单模型即可完成分类任务。

深度学习不再需要人工进行特征提取（例如选择西瓜的根蒂、色泽、响声等)，深度学习只需要输入西瓜的图片，就能自动提取特征。

#### 梯度爆炸和梯度消失

设fi是第i层的输出,同时也是i+1层的输入
$$
f_{i+1}(x) = f(wf_i(x)+b))\\
\frac{\partial f_{i+1}}{\partial x} = \frac{\partial f_{i+1}}{\partial f_i}\frac{\partial f_{i}}{\partial f_{i-1}}...\frac{\partial f_{1}}{\partial x}
$$


两个输出之间的求导，首先是对激活函数的求导，再去乘上ω。激活函数求导如果大于1，会造成梯度爆炸；激活函数求导小于1，会造成梯度消失。

解决方案

1. 预训练
2. 梯度裁剪。clamp函数控制梯度在一定范围内。
3. 正则化，Loss引入λ||w||²，如果||w||偏大，Loss会尽可能让它减小。

#### 卷积神经网络

[机器学习算法之——卷积神经网络(CNN)原理讲解 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/156926543)

卷积层：神经元是一个滤波器，在感受野范围内对上层的输入进行矩阵内积运算，得出结果后加上bias就是输出。**一个神经元都会有自己所关注的一个图像特征，所有神经元加起来就好比是整张图象的特征提取器集合**

池化层：压缩图像。压缩数据和参数的量，减少过拟合。

压缩的方法可以选择average或者max pooling。

###### 术语

卷积核：n*n的形式，表示算子的大小，卷积核对图片进行filter。不同的对应了不同的效果，或者说提取了不同的特征。

卷积核是训练出来的，像一个黑盒。提取的特征对人类来说没有那么只管。

stride步长：卷积核在原始数据上每次滑动的距离。

降采样：DownSampling，减少特征数量，舍弃了部分细节，降低了过拟合的可能。

#### CNN前向传播过程

图片裁剪乘同样大小

->CNN计算【RGB三通道分别于利用卷积核进行运算，得到的结果相加再加bias】

->Relu整流，超过1的归1，低于0的归0。

-> 新的CNN和Relu

-> Max Pooling，减小了特征的数量

-> 重复一遍CNN Relu Pooling

【注意CNN层可能会有很多卷积核，如果有10个就相当提取了10个特征】

->此时获得了池化后的10个特征

->Flatten，将池化后的特征展开成一维列向量。

-> 全连接层，计算每种分类的评估分数

->Softmax，以e为底再计算判定概率

## 第六章 支持向量机

SVM：一种线性二分类模型了，使得正反样例间隔最大。

分类学习最基本想法就是在样本空间找到一个划分超平面，将样本区分开。

但选择哪一个超平面最好是个问题？直觉上是寻找一个正中间的平面。

样本空间中的点到超平面（ω，b）的距离
$$
r = \frac{||\boldsymbol \omega^T \boldsymbol x+\boldsymbol b|| }{||\boldsymbol \omega||}
$$
如果能正确分类，对于正例，wx+b > 0 ；wx +b <0.

超平面满足的性质：

1. 正确区分正负样本。（对于yi=1的样本，wx+b >0；对于yi=-1的项wx+b<0）
2. 位于最近的正负样本(x+*,x-\*)中间，这样的话鲁棒性更强，更强泛化能力

可以列出不等式对w，b进行约束，但是由于等比例缩放，可以有无限组解，不妨再加一个约束。

满足x+*,x-\*的输出值恰好为+1和-1，最后就会导出下面这个式子。

![image-20230918001819659](C:\Users\卫清渠\AppData\Roaming\Typora\typora-user-images\image-20230918001819659.png)

支持向量指的是使得yi =+1 -1 取得的点，两个异类支持向量的距离被称为”margin“，距离是2/||ω||.最近的点距离是1/||ω||



我们的目标是找一个超平面，使得margin最大。

最小化||ω||，也就是最大化||ω||平方，同时保证

![image-20230918001951609](C:\Users\卫清渠\AppData\Roaming\Typora\typora-user-images\image-20230918001951609.png)

有复杂的数学知识可以计算出最优的w和b。

求解凸二次规划问题，考虑了它的对偶问题。还要满足KKT条件...

使用了比较高效的SMO算法求解。

#### 核函数

解决问题：存在无法用超平面分类的问题。

方法：将样本映射到更高维的特征空间，使得样本再高维特征空间内线性可分。

此时的模型可以表示为,φ是映射函数。
$$
f(x) = \omega^T\varphi(x)+b
$$
在求最优化的过程当中，需要就算x和x'在高位空间中的内积：φ(x)Tφ(x‘)。这个计算难度非常大，因为维数可能会非常高。为了避开这个障碍，构造一个函数
$$
k(x_i,x_j) =φ(x)^Tφ(x')
$$
这个k函数就是核函数。

我们希望样本在特征空间内线性可分，因此特征空间的好坏对SVM的性能至关重要。

核函数隐式的定义了这个特征空间，它成了SVM最大的变数。

![image-20230918011826367](C:\Users\卫清渠\AppData\Roaming\Typora\typora-user-images\image-20230918011826367.png)

#### 软间隔与正则化

问题：不一定存在超平面将不同类样本完全划分开。就算存在也可能是过拟合的效果（在训练集上确实可分，但是在样本空间内不一定可分）

缓解方法：允许SVM在一些样本上出错。

原本要求严格要求正例f(x) >=1  ，反例f(x) <= -1， 软间隔就是允许一些样本不满足约束。

![image-20230919002929069](C:\Users\卫清渠\AppData\Roaming\Typora\typora-user-images\image-20230919002929069.png)

优化目标改写如上，l0/1是01损失函数，内部是<0则+1，内部>=0则不计入。（这里就是处于硬间隔的数据，软间隔的计入）

C-> 无穷时就是强制要求软间隔内样本数为0.

01损失函数不光滑不连续，数学性质不太好。改用其它函数。

![image-20230919003312026](C:\Users\卫清渠\AppData\Roaming\Typora\typora-user-images\image-20230919003312026.png)

【正则化】：纠正规则，对误差函数引入纠正项，纠正项有L0,L1,L2类别之分。

目标：缓解过拟合。
$$
\lambda\Vert\theta\Vert ^q\ 为L_q正则项[\lambda > 0]
$$
L0正则项：L0（θ）为θ中非0参数个数。

L1正则项：L1（θ）θ各分量绝对值之和。

L2正则项：L2（θ）θ各分量平方之和

将正则项加入误差函数后，其对应的正则项参数的对应范数就会减小。

比如说使用L0，最终θ的0分量会更多，特征就会更稀疏。

L1：参数更趋向于0，类似L0

L2 : 相比于L0,L1函数光滑连续，可以求导，参数的模长会减小；大参数的模型数据一点点偏移就会引起很大影响。小参数的模型能够适应不同的数据集。

#### SVR支持向量回归

容忍f(x)与y之间至多有ε的误差，当f(x) -y 小于等于ε时，不计入误差，反之计入|result| - ε